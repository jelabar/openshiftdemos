Red Hat OpenShift Disaster Recovery Hub Cluster (ACM)
Status Messages
OpenShift Console: https://console-openshift-console.apps.cluster-ldbp4.ldbp4.sandbox119.opentlc.com
OpenShift API for command line 'oc' client: https://api.cluster-gqgh8.gqgh8.sandbox1620.opentlc.com:6443
Download oc client from http://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.12.12/openshift-client-linux-4.12.12.tar.gzAuthentication via htpasswd is enabled on this cluster.

User `admin` with password ` MTg5MDA5` is cluster admin.Users `user1` .. `user5` created with password ` MTg5MDA5`
You can access Gitea via the URL https://gitea.apps.cluster-ldbp4.ldbp4.sandbox119.opentlc.comThe 
Gitea admin username is 'dev-admin'.The Gitea admin password is 'openshift'.A Gitea user 'dev-user' was created, with the password 'openshift'

The following repositories were migrated for the created users:
https://github.com/AhilanPonnusamy/coolstore-RH-demo, 
https://github.com/redhat-gpte-devopsautomation/coolstore-argocd.git, 
https://github.com/RamenDR/ocm-ramen-samples.git, 
https://github.com/redhat-gpte-devopsautomation/globex.git


tmux a -t BusinessContinuity 

HUB Cluster
oc login --insecure-skip-tls-verify=false -u admin -p  MTg5MDA5 https://api.cluster-gqgh8.gqgh8.sandbox1620.opentlc.com:6443


Primary Cluster
oc login --insecure-skip-tls-verify=false -u admin -p  MTg5MDA5 https://api.cluster-gqgh8-1.sandbox334.opentlc.com:6443

Secondary Cluster
oc login --insecure-skip-tls-verify=false -u admin -p  MTg5MDA5 https://api.cluster-gqgh8-2.sandbox1465.opentlc.com:6443


Explicación Demo

1. Introduction
Regional-DR is composed of Red Hat Advanced Cluster Management for Kubernetes (RHACM) and OpenShift Data Foundation components to provide application and data mobility across OpenShift Container Platform clusters. It is built on Asynchronous data replication and hence could have a potential data loss but provides the protection against a broad set of failures.
This Regional-DR solution provides an automated "one-click" recovery in the event of a regional disaster. The protected applications are automatically redeployed to a designated OpenShift Container Platform with OpenShift Data Foundation cluster that is available in another region.
In this solution, Regional-DR leverages Red Hat OpenShift Data Foundation which is backed by Ceph as the storage provider, whose lifecycle is managed by Rook and it’s enhanced with the ability to:
* Enable pools for mirroring.
* Automatically mirror images across RBD pools.
* Provides csi-addons to manage per Persistent Volume Claim mirroring.
This release of Regional-DR supports Multi-Cluster configuration that is deployed across different regions and data centers. For example, a 2-way replication across two managed clusters located in two different regions or data centers.
The intent of this guide is to understand the implementation aspects necessary to be able to failover an application from one OpenShift Container Platform (OCP) cluster to another and then failback the same application to the original primary cluster. In this case the OCP clusters will be created or imported using Red Hat Advanced Cluster Management or RHACM.
This is a general overview of the steps required to configure and execute OpenShift Disaster Recovery (ODR) capabilities using OpenShift Data Foundation (ODF) v4.12 and RHACM v2.7 across two distinct OCP clusters separated by distance. In addition to these two cluster called managed clusters, there is currently a requirement to have a third OCP cluster that will be the Advanced Cluster Management (ACM) hub cluster.
	These steps are considered Tech Preview in ODF 4.12 and are provided for POC (Proof of Concept) purposes. OpenShift Regional Disaster Recovery will be supported for production usage in a future release.
Configuring OpenShift Data Foundation for Regional-DR with Advanced Cluster Management is a Technology Preview feature and is subject to Technology Preview support limitations. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

1.1. Regional-DR
In this lab we will focus on Regional-DR. Regional-DR is composed of Red Hat Advanced Cluster Management for Kubernetes and OpenShift Data Foundation components to provide application and data mobility across Red Hat OpenShift Container Platform clusters

1.2. Components of Regional-DR solution
1.2.1. Red Hat Advanced Cluster Management for Kubernetes
Red Hat Advanced Cluster Management (RHACM))provides the ability to manage multiple clusters and application lifecycles. Hence, it serves as a control plane in a multi-cluster environment.
RHACM is split into two parts:
* RHACM Hub: includes components that run on the multi-cluster control plane.
* Managed clusters: includes components that run on the clusters that are managed.

1.2.2. OpenShift Data Foundation
OpenShift Data Foundation provides the ability to provision and manage storage for stateful applications in an OpenShift Container Platform cluster.
OpenShift Data Foundation is backed by Ceph as the storage provider, whose lifecycle is managed by Rook in the OpenShift Data Foundation component stack. Ceph-CSI provides the provisioning and management of Persistent Volumes for stateful applications.
OpenShift Data Foundation stack is now enhanced with the following abilities for disaster recovery:
* Enable RBD block pools for mirroring across OpenShift Data Foundation instances (clusters)
* Ability to mirror specific images within an RBD block pool
* Provides csi-addons to manage per Persistent Volume Claim (PVC) mirroring
1.2.3. OpenShift DR
OpenShift DR is a set of orchestrators to configure and manage stateful applications across a set of peer OpenShift clusters which are managed using RHACM and provides cloud-native interfaces to orchestrate the life-cycle of an application’s state on Persistent Volumes. These include:
* Protecting an application and its state relationship across OpenShift clusters
* Failing over an application and its state to a peer cluster
* Relocate an application and its state to the previously deployed cluster
OpenShift DR is split into three components:
* ODF Multicluster Orchestrator: Installed on the multi-cluster control plane (RHACM Hub), it orchestrates configuration and peering of OpenShift Data Foundation clusters for Metro and Regional DR relationships
* OpenShift DR Hub Operator: Automatically installed as part of ODF Multicluster Orchestrator installation on the hub cluster to orchestrate failover or relocation of DR enabled applications.
* OpenShift DR Cluster Operator: Automatically installed on each managed cluster that is part of a Metro and Regional DR relationship to manage the lifecycle of all PVCs of an application.

1.3. Regional-DR deployment workflow
	Already implemented when this environment was provisioned. Here its mentioned for your understanding and exploration.
This section provides an overview of the steps required to configure and deploy Regional-DR capabilities using latest version of Red Hat OpenShift Data Foundation across two distinct OpenShift Container Platform clusters. In addition to two managed clusters, a third OpenShift Container Platform cluster will be required to deploy the Red Hat Advanced Cluster Management (RHACM)
This installation method requires you have three OpenShift clusters that have network reachability between them. For the purposes of this document we will use this reference for the clusters:
* Hub cluster is where ACM, ODF Multisite-orchestrator and ODR Hub controllers are installed.
* Primary managed cluster is where ODF, ODR Cluster controller, and Applications are installed.
* Secondary managed cluster is where ODF, ODR Cluster controller, and Applications are installed.

1.4. Steps just for understanding the implementation workflow
These steps are already executed for you during the lab setup except for the application onboarding which is the next lab.
1. Install the ACM operator on the hub cluster.After creating the OCP hub cluster, install from OperatorHub the ACM operator. After the operator and associated pods are running, create the MultiClusterHub resource.
2. Create or import managed OCP clusters into ACM hub.Import or create the two managed clusters with adequate resources for ODF (compute nodes, memory, cpu) using the RHACM console.
3. Ensure clusters have unique private network address ranges.Ensure the primary and secondary OCP clusters have unique private network address ranges.
4. Connect the private networks using Submariner add-ons.Connect the managed OCP private networks (cluster and service) using the RHACM Submariner add-ons.
5. Install ODF 4.12 on managed clusters.Install ODF 4.12 on primary and secondary OCP managed clusters and validate deployment.
6. Install ODF Multicluster Orchestrator on the ACM hub cluster.Install from OperatorHub on the ACM hub cluster the ODF Multicluster Orchestrator. The OpenShift DR Hub operator will also be installed.
7. Configure SSL access between S3 endpointsIf managed OpenShift clusters are not using valid certificates this step must be done by creating a new user-ca-bundle ConfigMap that contains the certs.
8. Create one or more DRPolicyUse the All Clusters Data Services UI to create DRPolicy by selecting the two managed clusters the policy will apply to.
9. Validate OpenShift DR Cluster operators are installed.Once the first DRPolicy is created this will trigger the DR Cluster operators to be created on the two managed clusters selected in the UI.
10. Following this we can setup an application using RHACM console and test failover/relocate.
    * Create an application using RHACM console for highly available application across regions.
    * Test failover and reolcate operations using the sampole application between managed clusters.

1.5. Review the implementation
Lets start by reviewing the implementation and ensuring that everything is working fine so that we can deploy an application onto OpenShift and achieve Business Continuity leveraging Regional-DR.
Logon to the Hub Cluster ACM console using your OpenShift credentials. Ensure that you use htpasswd_provide to login to OpenShift Console across all cluster’s for all the lab modules.

1.6. Verify Managed clusters are imported correctly
Select All Clusters and verify that you can see local and two managed clusters - primnary and secondary

1.7. Verify Managed clusters have non-overlapping networks
In order to connect the OpenShift cluster and service networks using the Submariner add-ons, it is necessary to validate the two clusters have non-overlapping networks. This can be done by running the following command for each of the managed clusters and check the spec section as shown below. Accept insecure connection as we know its the managed cluster in the lab environment.
For that you have a terminal window along with your workshop modules. You can use api login to respective cluster. If you want you can also ssh to each cluster separately using mutliple terminal windows outside of this browser based termninal window.


oc get networks.config.openshift.io cluster -o json | jq .spe



Deploy globex
https://github.com/redhat-gpte-devopsautomation/globex.git


4. Failover Application to secondary

Update Price
oc login -u admin -p  MTg5MDA5 https://api.cluster-gqgh8-1.sandbox334.opentlc.com:6443


POD=$(oc get pods -n globex | grep catalog-database | awk '{print $1}')
oc exec -it $POD -n globex -- psql --dbname catalog --command "update catalog set price = 15 where name = 'Quarkus T-shirt';"

Apply DRPolicy
PVC label:
app=globex


On Secondary Cluster create globex namespace y anotarlo con la suscripción de globex

oc login -u admin -p  MTg5MDA5 https://api.cluster-gqgh8-2.sandbox1465.opentlc.com:6443

oc new-project globex || oc project globex
oc annotate ns globex --overwrite=true apps.open-cluster-management.io/hosting-subscription=globex/globex-subscription-1

Failover Application on ACM
Verificar estado PVC en Primary

For production environment or real life scenario, the applications routes will be updated in Global Load Balancer using automated way for e.g. using ansible to update GLB’s with the new updated routes post failover / relocate.


Cambiar en segundo cluster y hacer Failback
oc login -u admin -p  MTg5MDA5 https://api.cluster-gqgh8-2.sandbox1465.opentlc.com:6443

POD=$(oc get pods -n globex | grep catalog-database | awk '{print $1}')
oc exec -it $POD -n globex -- psql --dbname catalog --command "update catalog set price = 20 where name = 'Quarkus T-shirt';"

Hacer click en relocate
Se demora un poco, verificar estado del mirroring.

oc login -u admin -p  MTg5MDA5 https://api.cluster-gqgh8-2.sandbox1465.opentlc.com:6443

oc get cephblockpool ocs-storagecluster-cephblockpool -n openshift-storage -o jsonpath='{.status.mirroringStatus.summary}{"\n"}'

oc get pvc -n globex



OnBoard a sample application and make it DR Ready

https://gitea.apps.cluster-ldbp4.ldbp4.sandbox119.opentlc.com/dev-user/busybox-app.git
namespace: busybox-app
path: busybox-odr
branch: main

Validate application running
Apply DRPolicy to sample application

Create namespace with annotations
oc login -u admin -p  MTg5MDA5 https://api.cluster-gqgh8-2.sandbox1465.opentlc.com:6443
oc new-project busybox-app || oc project busybox-app
oc annotate ns busybox-app --overwrite=true apps.open-cluster-management.io/hosting-subscription=busybox/busybox-subscription-1

Failover
Failback
